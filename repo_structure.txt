├── datasets/
│   ├── raw/                   # Store raw dataset files
│   ├── processed/             # Store preprocessed datasets (after tokenisation, etc.)
│   └── data_prep.py           # Script(s) for dataset creation/preparation
├── models/
│   ├── base/                  # Pre-trained model (Llama, etc.)
│   ├── fine-tuned/            # Fine-tuned models
│   └── fine_tuning.py         # Script for fine-tuning models
├── compression/
│   ├── quantisation/          # Codebase for applying quantisation
│   ├── pruning/               # Codebase for applying pruning
│   ├── distillation/          # Codebase for model distillation
│   └── apply_compression.py   # Pipeline for sequential compression (calls other techniques)
├── evaluation/
│   ├── translation_eval.py    # Scripts for BLEU, ROUGE, etc.
│   ├── perf_eval.py           # Scripts for speed, memory, and throughput evaluations
│   └── results/               # Folder for evaluation results and logs
├── notebooks/                 # Any exploratory or analysis Jupyter notebooks
├── utils/
│   ├── slurm_helpers.py       # SLURM job scheduling helper scripts
│   └── config.py              # Central configuration file for model, dataset, and evaluation parameters
├── requirements.txt           # List of dependencies
├── README.md                  # General description, instructions for setup, running, etc.
└── run_pipeline.py            # Main script to run entire pipeline (from fine-tuning, compression to evaluation)

> Note: not all these will be needed, just inspiration to get started with a well structured project.